# NanoNet

A tiny deep learning framework in Python built from scratch to understand core neural network concepts.


## **Implementation Checklist**

### **Core Components**
- [ ] **Tensor Class**
  - [ ] Multi-dimensional arrays (tensors)
  - [ ] Basic operations: addition, multiplication, reshaping, slicing

- [ ] **Computational Graph**
  - [ ] Nodes for operations
  - [ ] Edges for dependencies

- [ ] **Automatic Differentiation**
  - [ ] Forward pass
  - [ ] Backward pass (gradient calculation)
  - [ ] Gradient storage and application

### **Neural Network Layers**
- [ ] **Dense Layer** (Fully Connected)
  - [ ] Forward pass: matrix multiplication + bias
  - [ ] Backward pass: gradient calculation for weights and biases

- [ ] **Activation Functions**
  - [ ] ReLU
  - [ ] Sigmoid
  - [ ] Tanh

- [ ] **Loss Functions**
  - [ ] Mean Squared Error (MSE)
  - [ ] Cross-Entropy Loss

### **Optimization**
- [ ] **Gradient Descent**
  - [ ] Parameter updates using gradients

- [ ] **Advanced Optimizers** (optional)
  - [ ] Adam
  - [ ] RMSProp

### **Training Loop**
- [ ] **Epoch Management**
  - [ ] Forward pass
  - [ ] Loss computation
  - [ ] Backward pass
  - [ ] Parameter updates

- [ ] **Batch Processing** (optional)
  - [ ] Mini-batch gradient descent

### **Utilities**
- [ ] **Model Initialization**
  - [ ] Weight initialization (e.g., random, Xavier)

- [ ] **Model Saving/Loading** (optional)
  - [ ] Save and load model weights

### **Testing and Debugging**
- [ ] **Unit Tests**
  - [ ] Test tensor operations
  - [ ] Test gradient calculations

- [ ] **Example Implementations**
  - [ ] Simple models (e.g., single-layer perceptron, basic CNN)

### **Documentation**
- [ ] **Usage Instructions**
  - [ ] Installation
  - [ ] Basic examples

- [ ] **API Reference**
  - [ ] Classes and methods
  - [ ] Code comments

